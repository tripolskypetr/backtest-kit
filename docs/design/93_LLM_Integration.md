# LLM Integration

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [src/classes/Optimizer.ts](src/classes/Optimizer.ts)
- [src/client/ClientOptimizer.ts](src/client/ClientOptimizer.ts)
- [src/interfaces/Optimizer.interface.ts](src/interfaces/Optimizer.interface.ts)
- [src/lib/services/connection/OptimizerConnectionService.ts](src/lib/services/connection/OptimizerConnectionService.ts)
- [src/lib/services/global/OptimizerGlobalService.ts](src/lib/services/global/OptimizerGlobalService.ts)
- [src/lib/services/schema/OptimizerSchemaService.ts](src/lib/services/schema/OptimizerSchemaService.ts)
- [src/lib/services/template/OptimizerTemplateService.ts](src/lib/services/template/OptimizerTemplateService.ts)
- [src/lib/services/validation/OptimizerValidationService.ts](src/lib/services/validation/OptimizerValidationService.ts)
- [src/model/Message.model.ts](src/model/Message.model.ts)

</details>



This page documents the integration between backtest-kit's Optimizer system and Large Language Models via the Ollama API. The integration is implemented through three core helper functions (`text()`, `json()`, `dumpJson()`) that are generated by `OptimizerTemplateService` and included in every optimizer-generated strategy file.

The LLM integration enables automated strategy generation by processing historical market data through multi-timeframe analysis and producing structured trading signals via JSON schema enforcement.

For information about the overall optimizer architecture, see [Optimizer Architecture](#16.5.1). For details on data source pagination, see [Data Collection Pipeline](#16.5.2). For the complete code generation process, see [Strategy Code Generation](#16.5.4).

---

## Ollama API Integration

The optimizer integrates with **Ollama** hosting the `deepseek-v3.1:671b` model. Authentication uses the `OLLAMA_API_KEY` environment variable. The integration consists of three template methods in `OptimizerTemplateService`:

| Template Method | Generated Function | Purpose |
|-----------------|-------------------|---------|
| `getTextTemplate()` | `text(messages)` | Unstructured market analysis |
| `getJsonTemplate()` | `json(messages)` | Structured signal generation with schema |
| `getJsonDumpTemplate()` | `dumpJson(resultId, history, result)` | Debug output to `./dump/strategy/{uuid}/` |
</thinking>

**Sources:** [src/lib/services/template/OptimizerTemplateService.ts:1-27](), [src/lib/services/template/OptimizerTemplateService.ts:452-546](), [src/lib/services/template/OptimizerTemplateService.ts:555-612](), [src/lib/services/template/OptimizerTemplateService.ts:629-712]()

**LLM Integration Data Flow**

```mermaid
graph TB
    subgraph "ClientOptimizer Data Collection"
        GET_STRATEGY_DATA_FN["GET_STRATEGY_DATA_FN<br/>Iterates rangeTrain"]
        RESOLVE_PAGINATION_FN["RESOLVE_PAGINATION_FN<br/>iterateDocuments<br/>distinctDocuments"]
        DEFAULT_USER_FN["DEFAULT_USER_FN<br/>source.user()"]
        DEFAULT_ASSISTANT_FN["DEFAULT_ASSISTANT_FN<br/>source.assistant()"]
    end
    
    subgraph "OptimizerTemplateService"
        getUserMessage["getUserMessage()<br/>Default formatter"]
        getAssistantMessage["getAssistantMessage()<br/>Default formatter"]
        getTextTemplate["getTextTemplate()<br/>Generates text() helper"]
        getJsonTemplate["getJsonTemplate()<br/>Generates json() helper"]
        getJsonDumpTemplate["getJsonDumpTemplate()<br/>Generates dumpJson() helper"]
    end
    
    subgraph "Generated Strategy Code"
        getSignal["addStrategy.getSignal<br/>Multi-timeframe analysis"]
        text_helper["async text(messages)"]
        json_helper["async json(messages)"]
        dumpJson_helper["async dumpJson(resultId, history, result)"]
    end
    
    subgraph "Ollama API"
        ollama_chat["ollama.chat()<br/>model: deepseek-v3.1:671b<br/>host: https://ollama.com"]
        json_format["format: JSON schema<br/>position, priceOpen,<br/>priceTakeProfit, priceStopLoss"]
    end
    
    GET_STRATEGY_DATA_FN --> RESOLVE_PAGINATION_FN
    RESOLVE_PAGINATION_FN --> DEFAULT_USER_FN
    RESOLVE_PAGINATION_FN --> DEFAULT_ASSISTANT_FN
    DEFAULT_USER_FN --> getUserMessage
    DEFAULT_ASSISTANT_FN --> getAssistantMessage
    
    getTextTemplate --> text_helper
    getJsonTemplate --> json_helper
    getJsonDumpTemplate --> dumpJson_helper
    
    getSignal --> json_helper
    json_helper --> ollama_chat
    json_helper --> json_format
    json_format --> ollama_chat
    ollama_chat --> dumpJson_helper
```

**Sources:** [src/client/ClientOptimizer.ts:99-215](), [src/lib/services/template/OptimizerTemplateService.ts:1-27](), [src/lib/services/template/OptimizerTemplateService.ts:168-304]()

---

## Message Construction in ClientOptimizer

The `GET_STRATEGY_DATA_FN` function in `ClientOptimizer` builds conversation history by iterating through `rangeTrain` and calling `RESOLVE_PAGINATION_FN` for each source. Each source contributes a user-assistant message pair using either custom formatters or defaults from `OptimizerTemplateService`.

**Message Construction Flow in GET_STRATEGY_DATA_FN**

```mermaid
graph TB
    START["GET_STRATEGY_DATA_FN(symbol, self)"]
    ITERATE_RANGES["for (rangeTrain)"]
    ITERATE_SOURCES["for (source)"]
    CHECK_TYPE["typeof source === 'function'"]
    
    subgraph "Function Source Branch"
        FETCH_FN["RESOLVE_PAGINATION_FN(source, filterData)"]
        USER_DEFAULT["DEFAULT_USER_FN(symbol, data, name, self)"]
        ASST_DEFAULT["DEFAULT_ASSISTANT_FN(symbol, data, name, self)"]
    end
    
    subgraph "Object Source Branch"
        FETCH_OBJ["RESOLVE_PAGINATION_FN(source.fetch, filterData)"]
        USER_CUSTOM["source.user(symbol, data, name, self)"]
        ASST_CUSTOM["source.assistant(symbol, data, name, self)"]
    end
    
    PUSH_MSG["messageList.push({role: 'user', content},<br/>{role: 'assistant', content})"]
    GET_PROMPT["getPrompt(symbol, messageList)"]
    STRATEGY_OBJ["strategyList.push({symbol, name,<br/>messages, strategy})"]
    
    START --> ITERATE_RANGES
    ITERATE_RANGES --> ITERATE_SOURCES
    ITERATE_SOURCES --> CHECK_TYPE
    CHECK_TYPE -->|true| FETCH_FN
    CHECK_TYPE -->|false| FETCH_OBJ
    FETCH_FN --> USER_DEFAULT
    FETCH_FN --> ASST_DEFAULT
    FETCH_OBJ --> USER_CUSTOM
    FETCH_OBJ --> ASST_CUSTOM
    USER_DEFAULT --> PUSH_MSG
    ASST_DEFAULT --> PUSH_MSG
    USER_CUSTOM --> PUSH_MSG
    ASST_CUSTOM --> PUSH_MSG
    PUSH_MSG --> GET_PROMPT
    GET_PROMPT --> STRATEGY_OBJ
```

**Sources:** [src/client/ClientOptimizer.ts:99-215]()

**DEFAULT_USER_FN and DEFAULT_ASSISTANT_FN Implementation**

These functions delegate to `OptimizerTemplateService` methods:

| Function | Signature | Delegates To |
|----------|-----------|--------------|
| `DEFAULT_USER_FN` | `(symbol, data, name, self) => Promise<string>` | `self.params.template.getUserMessage(symbol, data, name)` |
| `DEFAULT_ASSISTANT_FN` | `(symbol, data, name, self) => Promise<string>` | `self.params.template.getAssistantMessage(symbol, data, name)` |

**Sources:** [src/client/ClientOptimizer.ts:34-60]()

**MessageModel Interface**

The `MessageModel` type from `src/model/Message.model.ts` defines the conversation structure:

```typescript
interface MessageModel {
  role: "assistant" | "system" | "user";
  content: string;
}
```

**Sources:** [src/model/Message.model.ts:1-26]()

---

## Helper Function Generation

`OptimizerTemplateService` generates three helper functions that are injected into every optimizer-created strategy file. These functions encapsulate Ollama API calls with consistent configuration.

**Generated Helper Functions**

| Template Method | Generated Function | Purpose | Return Type |
|-----------------|-------------------|---------|-------------|
| `getTextTemplate(symbol)` | `async text(messages)` | Unstructured market analysis | `Promise<string>` |
| `getJsonTemplate(symbol)` | `async json(messages)` | Structured signal with JSON schema | `Promise<object>` |
| `getJsonDumpTemplate(symbol)` | `async dumpJson(resultId, history, result)` | Debug logging to disk | `Promise<void>` |

**Ollama Client Configuration in Generated Code**

Both `text()` and `json()` helpers instantiate the Ollama client identically:

```javascript
const ollama = new Ollama({
    host: "https://ollama.com",
    headers: {
        Authorization: `Bearer ${process.env.OLLAMA_API_KEY}`,
    },
});
```

**Sources:** [src/lib/services/template/OptimizerTemplateService.ts:555-574](), [src/lib/services/template/OptimizerTemplateService.ts:629-640]()

**getTextTemplate() - Unstructured Analysis**

The `getTextTemplate()` method generates a `text(messages)` helper for fundamental market analysis. The function prepends system instructions and appends a final user query requesting entry conditions.

**text() Function Structure**

```mermaid
graph TB
    text_call["text(messages)"]
    ollama_init["ollama = new Ollama({host, headers})"]
    system_msg["Prepend system message:<br/>'В ответ напиши торговую стратегию<br/>где нет ничего лишнего'"]
    user_final["Append user message:<br/>'На каких условиях мне купить {symbol}?<br/>Дай анализ рынка на основе<br/>поддержки/сопротивления'"]
    ollama_chat["response = await ollama.chat({<br/>model: 'deepseek-v3.1:671b',<br/>messages: [system, ...messages, user]<br/>})"]
    extract["content = response.message.content.trim()"]
    escape["return content with escaped<br/>backslashes, backticks, dollars, quotes"]
    
    text_call --> ollama_init
    ollama_init --> system_msg
    system_msg --> user_final
    user_final --> ollama_chat
    ollama_chat --> extract
    extract --> escape
```

**Sources:** [src/lib/services/template/OptimizerTemplateService.ts:555-612]()

**System Message Content**

The system message instructs the LLM to produce copy-paste ready output:

```
В ответ напиши торговую стратегию где нет ничего лишнего,
только отчёт готовый для копипасты целиком

**ВАЖНО**: Не здоровайся, не говори что делаешь - только отчёт!
```

**Final User Message Template**

The function appends a final user query with the symbol injected via `escapedSymbol`:

```
На каких условиях мне купить ${escapedSymbol}?
Дай анализ рынка на основе поддержки/сопротивления, точек входа в LONG/SHORT позиции.
Какой RR ставить для позиций?
Предпочтительны LONG или SHORT позиции?

Сделай не сухой технический, а фундаментальный анализ, содержащий стратигическую рекомендацию
```

**Sources:** [src/lib/services/template/OptimizerTemplateService.ts:581-599]()

**getJsonTemplate() - Structured Signal Generation**

The `getJsonTemplate()` method generates a `json(messages)` helper that enforces a JSON schema for trading signals. This is the primary function called in generated `addStrategy.getSignal()` implementations.

**json() Function Structure**

```mermaid
graph TB
    json_call["json(messages)"]
    ollama_init["ollama = new Ollama({host, headers})"]
    system_msg["Prepend system message with<br/>trading rules (position types,<br/>price levels, time estimates)"]
    format_schema["Define format object:<br/>{type: 'object', properties: {...},<br/>required: [6 fields]}"]
    ollama_chat["response = await ollama.chat({<br/>model: 'deepseek-v3.1:671b',<br/>messages: [system, ...messages],<br/>format: format_schema<br/>})"]
    parse_json["jsonResponse = JSON.parse(<br/>response.message.content.trim()<br/>)"]
    return["return jsonResponse"]
    
    json_call --> ollama_init
    ollama_init --> system_msg
    system_msg --> format_schema
    format_schema --> ollama_chat
    ollama_chat --> parse_json
    parse_json --> return
```

**Sources:** [src/lib/services/template/OptimizerTemplateService.ts:629-712]()

**ollama.chat() Parameters**

The `json()` helper calls `ollama.chat()` with the `format` parameter to enforce schema:

| Parameter | Value |
|-----------|-------|
| `model` | `"deepseek-v3.1:671b"` |
| `messages` | `[{role: "system", content}, ...messages]` |
| `format` | JSON schema object with 6 required properties |

**Sources:** [src/lib/services/template/OptimizerTemplateService.ts:642-706]()

---

## Multi-Timeframe Analysis in getStrategyTemplate()

The `getStrategyTemplate()` method generates `addStrategy()` calls where `getSignal()` implements a progressive multi-timeframe analysis. The function calls `getCandles()` four times and builds a `messages` array with five user-assistant message pairs before calling `json(messages)`.

**getSignal() Execution Flow**

```mermaid
graph TB
    getSignal_start["getSignal: async (symbol) => {...}"]
    init_messages["const messages = []"]
    
    subgraph "Data Loading Phase"
        fetch_1m["microTermCandles = await getCandles(symbol, '1m', 30)"]
        fetch_5m["mainTermCandles = await getCandles(symbol, '5m', 24)"]
        fetch_15m["shortTermCandles = await getCandles(symbol, '15m', 24)"]
        fetch_1h["mediumTermCandles = await getCandles(symbol, '1h', 24)"]
    end
    
    subgraph "Message Building Phase"
        msg1["messages.push({user: '1h analysis'},<br/>{assistant: 'Тренд 1h проанализирован'})"]
        msg2["messages.push({user: '15m analysis'},<br/>{assistant: 'Тренд 15m проанализирован'})"]
        msg3["messages.push({user: '5m analysis'},<br/>{assistant: 'Таймфрейм 5m проанализирован'})"]
        msg4["messages.push({user: '1m analysis'},<br/>{assistant: 'Микроструктура 1m проанализирована'})"]
        msg5["messages.push({user: strategy prompt<br/>with escapedPrompt})"]
    end
    
    uuid_gen["const resultId = uuid()"]
    json_call["const result = await json(messages)"]
    dump_call["await dumpJson(resultId, messages, result)"]
    set_id["result.id = resultId"]
    return["return result"]
    
    getSignal_start --> init_messages
    init_messages --> fetch_1m
    init_messages --> fetch_5m
    init_messages --> fetch_15m
    init_messages --> fetch_1h
    fetch_1h --> msg1
    msg1 --> msg2
    msg2 --> msg3
    msg3 --> msg4
    msg4 --> msg5
    msg5 --> uuid_gen
    uuid_gen --> json_call
    json_call --> dump_call
    dump_call --> set_id
    set_id --> return
```

**Sources:** [src/lib/services/template/OptimizerTemplateService.ts:194-302]()

**Timeframe Parameters**

| Variable Name | Interval | Count | Comment in Code |
|---------------|----------|-------|-----------------|
| `mediumTermCandles` | `"1h"` | `24` | `// Сообщение 1: Среднесрочный тренд` |
| `shortTermCandles` | `"15m"` | `24` | `// Сообщение 2: Краткосрочный тренд` |
| `mainTermCandles` | `"5m"` | `24` | `// Сообщение 3: Основной таймфрейм` |
| `microTermCandles` | `"1m"` | `30` | `// Сообщение 4: Микро-структура` |

**Sources:** [src/lib/services/template/OptimizerTemplateService.ts:202-205]()

**formatCandles() Local Function**

The generated code includes a local `formatCandles()` function for consistent OHLCV formatting:

```javascript
function formatCandles(candles, timeframe) {
    return candles.map((c) =>
        `${new Date(c.timestamp).toISOString()}[${timeframe}]: O:${c.open} H:${c.high} L:${c.low} C:${c.close} V:${c.volume}`
    ).join("\n");
}
```

**Sources:** [src/lib/services/template/OptimizerTemplateService.ts:207-211]()

---

## JSON Schema Enforcement in json() Helper

The `json()` helper function passes a `format` parameter to `ollama.chat()` that defines a strict JSON schema. Ollama's JSON mode ensures the LLM output conforms to the schema structure.

**ollama.chat() format Parameter Structure**

```mermaid
graph TB
    format_obj["format: { type: 'object' }"]
    properties["properties: {}"]
    required["required: string[]"]
    
    subgraph "Six Required Properties"
        position["position:<br/>{type: 'string',<br/>enum: ['wait', 'long', 'short'],<br/>description: 'Trade decision'}"]
        note["note:<br/>{type: 'string',<br/>description: 'Professional recommendation'}"]
        priceOpen["priceOpen:<br/>{type: 'number',<br/>description: 'Entry price'}"]
        priceTP["priceTakeProfit:<br/>{type: 'number',<br/>description: 'Take profit target'}"]
        priceSL["priceStopLoss:<br/>{type: 'number',<br/>description: 'Stop loss exit'}"]
        timeEst["minuteEstimatedTime:<br/>{type: 'number',<br/>description: 'Expected duration (max 360)'}"]
    end
    
    format_obj --> properties
    format_obj --> required
    properties --> position
    properties --> note
    properties --> priceOpen
    properties --> priceTP
    properties --> priceSL
    properties --> timeEst
```

**Sources:** [src/lib/services/template/OptimizerTemplateService.ts:675-705]()

**Schema Property Definitions**

The schema enforces six required fields with specific types and descriptions:

| Property Name | Type | Enum Values | Description Field |
|---------------|------|-------------|-------------------|
| `position` | `"string"` | `["wait", "long", "short"]` | `"Trade decision: wait (no signal), long (buy), or short (sell)"` |
| `note` | `"string"` | - | `"Professional trading recommendation with price levels"` |
| `priceOpen` | `"number"` | - | `"Entry price (current market price or limit order price)"` |
| `priceTakeProfit` | `"number"` | - | `"Take profit target price"` |
| `priceStopLoss` | `"number"` | - | `"Stop loss exit price"` |
| `minuteEstimatedTime` | `"number"` | - | `"Expected time to reach TP in minutes (max 360)"` |

**Sources:** [src/lib/services/template/OptimizerTemplateService.ts:677-703]()

**System Message Content**

The system message in `json()` provides detailed trading rules split into four sections:

| Section | Key Content |
|---------|-------------|
| ТИПЫ ПОЗИЦИЙ | `position='wait'`: no signal, `position='long'`: bullish, `position='short'`: bearish |
| ЦЕНА ВХОДА (priceOpen) | Can be current market price or limit order price based on technical analysis |
| УРОВНИ ВЫХОДА | LONG: `priceTakeProfit > priceOpen > priceStopLoss`, SHORT: `priceStopLoss > priceOpen > priceTakeProfit` |
| ВРЕМЕННЫЕ РАМКИ | `minuteEstimatedTime` based on ATR, ADX, MACD, Momentum, Slope (max 360) |

**Sources:** [src/lib/services/template/OptimizerTemplateService.ts:647-671]()

---

## Template Override System

`OptimizerTemplateService` provides defaults for all `IOptimizerTemplate` methods. These can be overridden at the schema level (`IOptimizerSchema.template`) or per-source level (`IOptimizerSource.user`/`assistant`).

**Template Method Resolution in OptimizerConnectionService**

```mermaid
graph TB
    getOptimizer["OptimizerConnectionService.getOptimizer(optimizerName)"]
    get_schema["optimizerSchemaService.get(optimizerName)"]
    extract_template["Extract schema.template (rawTemplate)"]
    
    subgraph "Template Merging"
        defaults["OptimizerTemplateService defaults"]
        override_check["For each IOptimizerTemplate method"]
        use_custom["rawTemplate.method || defaults.method"]
        merged["Complete IOptimizerTemplate object"]
    end
    
    new_client["new ClientOptimizer({...params,<br/>template: merged})"]
    
    getOptimizer --> get_schema
    get_schema --> extract_template
    extract_template --> defaults
    defaults --> override_check
    override_check --> use_custom
    use_custom --> merged
    merged --> new_client
```

**Sources:** [src/lib/services/connection/OptimizerConnectionService.ts:59-113]()

**IOptimizerTemplate Method List**

The service provides defaults for eleven template methods:

| Method Name | Default Return | Purpose |
|-------------|----------------|---------|
| `getUserMessage(symbol, data, name)` | `"Прочитай данные и скажи ОК\n\n" + JSON.stringify(data)` | User message formatter |
| `getAssistantMessage(symbol, data, name)` | `"ОК"` | Assistant message formatter |
| `getTopBanner(symbol)` | Shebang + imports + WARN_KB | File header |
| `getTextTemplate(symbol)` | `async function text(messages) {...}` | LLM text analysis helper |
| `getJsonTemplate(symbol)` | `async function json(messages) {...}` | LLM JSON schema helper |
| `getJsonDumpTemplate(symbol)` | `async function dumpJson(...) {...}` | Debug output helper |
| `getExchangeTemplate(symbol, exchangeName)` | `addExchange({...})` | CCXT Binance integration |
| `getFrameTemplate(symbol, frameName, ...)` | `addFrame({...})` | Timeframe configuration |
| `getStrategyTemplate(strategyName, interval, prompt)` | `addStrategy({...})` | Strategy with getSignal |
| `getWalkerTemplate(walkerName, ...)` | `addWalker({...})` | Strategy comparison |
| `getLauncherTemplate(symbol, walkerName)` | `Walker.background(...) + listeners` | Execution and event handling |

**Sources:** [src/lib/services/template/OptimizerTemplateService.ts:27-716]()

**Override Levels**

Custom formatters can be provided at two granularities:

| Level | Interface | Override Fields | Scope |
|-------|-----------|-----------------|-------|
| Schema Level | `IOptimizerSchema.template` | `Partial<IOptimizerTemplate>` | All sources in this optimizer |
| Source Level | `IOptimizerSource.user/assistant` | `(symbol, data, name) => string \| Promise<string>` | Specific source only |

**Sources:** [src/interfaces/Optimizer.interface.ts:129-177](), [src/interfaces/Optimizer.interface.ts:426-427]()

---

## Debug Logging with dumpJson()

The `getJsonDumpTemplate()` method generates an `async dumpJson(resultId, history, result, outputDir)` function that writes conversation history and LLM output to `./dump/strategy/{resultId}/`. This function is called in every generated `getSignal()` implementation.

**dumpJson() Execution Flow**

```mermaid
graph TB
    dumpJson_call["dumpJson(resultId, history, result, outputDir='./dump/strategy')"]
    filter_msgs["systemMessages = history.filter(m => m.role === 'system')<br/>userMessages = history.filter(m => m.role === 'user')"]
    subfolderPath["subfolderPath = path.join(outputDir, resultId)"]
    check_exists["await fs.access(subfolderPath)"]
    exists["Folder exists"]
    not_exists["Folder does not exist"]
    mkdir["await fs.mkdir(subfolderPath, {recursive: true})"]
    
    subgraph "File Writing Phase"
        write_system["Write 00_system_prompt.md<br/>ResultId + Output JSON + System messages"]
        write_user["Write 01_user_message.md, 02_, etc.<br/>Promise.all(userMessages.map(...))<br/>Check message size vs WARN_KB"]
        write_output["Write XX_llm_output.md<br/>ResultId + Full output JSON"]
    end
    
    dumpJson_call --> filter_msgs
    filter_msgs --> subfolderPath
    subfolderPath --> check_exists
    check_exists -->|success| exists
    check_exists -->|error| not_exists
    exists --> dumpJson_call
    not_exists --> mkdir
    mkdir --> write_system
    write_system --> write_user
    write_user --> write_output
```

**Sources:** [src/lib/services/template/OptimizerTemplateService.ts:457-544]()

**Generated File Structure**

| Filename Pattern | Content Description | Line Range |
|------------------|---------------------|------------|
| `00_system_prompt.md` | ResultId, output JSON, all system messages | [src/lib/services/template/OptimizerTemplateService.ts:470-498]() |
| `01_user_message.md`, `02_user_message.md`, ... | ResultId, user input N, size check via `Buffer.byteLength()` | [src/lib/services/template/OptimizerTemplateService.ts:500-525]() |
| `XX_llm_output.md` (where XX = userMessages.length + 1) | ResultId, full output JSON | [src/lib/services/template/OptimizerTemplateService.ts:527-543]() |

**Sources:** [src/lib/services/template/OptimizerTemplateService.ts:452-546]()

**WARN_KB Threshold**

The constant `WARN_KB = 100` (KB) is checked for each user message:

```javascript
const messageSizeBytes = Buffer.byteLength(message.content, "utf8");
const messageSizeKb = Math.floor(messageSizeBytes / 1024);
if (messageSizeKb > WARN_KB) {
    console.warn(
        `User message ${idx + 1} is ${messageSizeBytes} bytes (${messageSizeKb}kb), which exceeds warning limit`
    );
}
```

**Sources:** [src/lib/services/template/OptimizerTemplateService.ts:64](), [src/lib/services/template/OptimizerTemplateService.ts:508-515]()

**Return Early Logic**

If the `resultId` folder already exists, `dumpJson()` returns immediately without writing files. This prevents overwriting debug output on subsequent calls with the same UUID.

**Sources:** [src/lib/services/template/OptimizerTemplateService.ts:463-467]()

---

## Integration with Strategy Execution

The generated LLM-based strategies integrate seamlessly with the backtest-kit execution framework. The `getSignal()` function calls `json(messages)` to get trading signals, which are then validated and executed.

**Signal Execution Flow**

```mermaid
sequenceDiagram
    participant Strategy as ClientStrategy
    participant GetSignal as getSignal()
    participant LLM as json(messages)
    participant Dump as dumpSignal()
    participant Validation as RiskValidation
    
    Strategy->>GetSignal: Call with symbol
    GetSignal->>GetSignal: Build multi-timeframe messages
    GetSignal->>LLM: json(messages)
    LLM-->>GetSignal: { position, prices, time }
    GetSignal->>GetSignal: Generate UUID
    GetSignal->>Dump: dumpSignal(uuid, messages, result)
    Dump-->>GetSignal: Debug files written
    GetSignal->>GetSignal: Set result.id = uuid
    GetSignal-->>Strategy: Return signal
    Strategy->>Validation: Validate signal
    Validation-->>Strategy: Accept/Reject
```

**Generated Strategy Example**

The optimizer generates strategies that follow this pattern:

```javascript
addStrategy({
    strategyName: "xyz_strategy-1",
    interval: "5m",
    getSignal: async (symbol) => {
        const messages = [];
        
        // 1-4: Multi-timeframe data loading
        const microTermCandles = await getCandles(symbol, "1m", 30);
        const mainTermCandles = await getCandles(symbol, "5m", 24);
        // ... build messages array
        
        // 5: Request signal with strategy prompt
        messages.push({ role: "user", content: strategyPrompt });
        
        const resultId = uuid();
        const result = await json(messages);
        await dumpJson(resultId, messages, result);
        
        result.id = resultId;
        return result;
    },
});
```

**Sources:** [demo/backtest/src/index.mjs:91-106](), [demo/live/src/index.mjs:87-102](), [src/lib/services/template/OptimizerTemplateService.ts:168-304]()